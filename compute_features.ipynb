{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f95bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from utils import Transform, ImageTextDataset, collate_fn, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86fcae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the open CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"./out/m1\").to(device)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    ImageTextDataset('data', \"valid\", transform=Transform(224, False)),\n",
    "    batch_size=1,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "039df360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that computes the feature vectors for a batch of images\n",
    "def image_features(batch) -> np.ndarray:\n",
    "    with torch.no_grad():\n",
    "        # Encode the photos batch to compute the feature vectors and normalize them\n",
    "        features = model.get_image_features(batch['pixel_values'].to(device))\n",
    "        features = F.normalize(features, dim=-1)\n",
    "\n",
    "    return features.cpu().numpy()\n",
    "\n",
    "def save_image_features(save_name: str = \"25k_features.npy\") -> None:\n",
    "    all_features = []\n",
    "    for batch in tqdm(test_loader):\n",
    "        features = image_features(batch)\n",
    "        all_features.append(features)\n",
    "    all_features = np.stack(all_features).squeeze()\n",
    "    np.save(save_name, all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8ba800d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f392ee07d4534aba98ae0c7f41cd438c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_image_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0cb04de",
   "metadata": {},
   "outputs": [],
   "source": [
    "precomputed_image_embeddings = np.load(\"25k_features.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aa0acd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
