# Searching Images: From Clip And Beyond

## Authors

- [Meng Wang](https://github.com/Moenupa)
- [Yanlong Zhao](https://github.com/Ryan-Yanlong)

## Abstract

Nowadays, the increasing volume of multi-modal data has led to a heightened interest in image-text retrieval (ITR). To enhance the relevance, efficiency, scalability, and convenience of image search engines, our study explores advancements in this field by focusing on the CLIP architecture. We fine-tune CLIP on the Unsplash Lite Dataset, integrate it with various text encoders, and extend searches to dynamic databases using the Unsplash API. Experiments demonstrate enhanced search relevance and performance in both scoped and open search modes. Finally, we compare these approaches by analyzing their performance and computational costs.

## Note

This code and models are publicly available on [github](https://github.com/Moenupa/clip-image-search), you may find models in [release page](https://github.com/Moenupa/clip-image-search/releases).